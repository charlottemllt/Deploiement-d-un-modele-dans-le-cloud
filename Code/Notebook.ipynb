{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/ubuntu/spark-3.3.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a52c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660479dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.0')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "\n",
    "# User 1\n",
    "access_key = \"AKIAQ6ET7AAE7KUI7PUU\"\n",
    "secret_key = \"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\"\n",
    "\n",
    "# access_key = \"AKIAQ6ET7AAESVDSBNPZ\"\n",
    "# secret_key = \"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\"\n",
    "\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', access_key)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', secret_key)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4035f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import split, element_at, col, pandas_udf, PandasUDFType\n",
    "\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373a5f6",
   "metadata": {},
   "source": [
    "## Chargement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_categorie(path):\n",
    "    '''Renvoie la catégorie d\\'une image à partir de son chemin'''\n",
    "    if len(path) > 0:\n",
    "        #catégorie de l'image\n",
    "        return path.split('/')[-2]\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "def load_data(path_img):\n",
    "    '''Chargement des dataframes: \n",
    "    Prend en entrée le répertoire qui contient les sous répertoires contenant les images\n",
    "    Renvoie en sortie un spark dataframe contenant les images et \n",
    "    un spark dataframe contenant les noms des fruits associés'''\n",
    "    #compteur\n",
    "    start = time.time()\n",
    "    #chargement dataframe des images\n",
    "\n",
    "    df_img = spark.read.format(\"binaryFile\").option(\"recursiveFileLookup\",\"true\").load(path_img) #.option(\"pathGlobFilter\", \"*.jpg\") # ne fonctionne pas si il y a des espaces dans le chemin\n",
    "    #df_img =  ImageSchema.readImages(path_img, dropImageFailures = True)\n",
    "    print('Chargement effectué')\n",
    "    #récupération chemin à partir des images\n",
    "    df_img = df_img.withColumn(\"path\", input_file_name())\n",
    "    #catégorisation des images\n",
    "    udf_categorie = udf(parse_categorie, StringType())\n",
    "    df_img = df_img.withColumn('Catégorie', udf_categorie('path'))\n",
    "    print('Temps de chargement des images : {} secondes'.format(time.strftime('%S', time.gmtime(time.time()-start))))\n",
    "    \n",
    "    return df_img\n",
    "\n",
    "def preprocess_data(dataframe):\n",
    "    '''Renvoie le résultat de l'avant dernière couche de chaque image du dataframe via transform du ResNet50\n",
    "    return un df contenant des vecteurs de dimension 1x2048 '''\n",
    "    \n",
    "    from sparkdl import DeepImageFeaturizer\n",
    "    # DeepImageFeaturizer Applies the model specified by its popular name, \n",
    "    # with its prediction layer(s) chopped off\n",
    "    featurizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"image_preprocessed\", modelName=\"VGG16\")\n",
    "    output = featurizer.transform(dataframe).select(['path', 'Catégorie', 'image_preprocessed'])\n",
    "    del featurizer\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3a://donnees-projet8/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc26517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io as io\n",
    "import requests\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = spark_df.select('content').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a78177",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img = img[0][0]\n",
    "Image.open(io.BytesIO(first_img ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0410829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0780c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1249ad",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = spark_df.select('content').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74830883",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_img_to_array = lambda rawdata: np.asarray(Image.open(io.BytesIO(rawdata[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = list(map(binary_img_to_array, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d689123",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_image=[]\n",
    "for i in range(len(img_list)):\n",
    "    tmp_img=resize(img_list[i],output_shape=(224,224),order=2,anti_aliasing=True)\n",
    "    tmp_img=np.expand_dims(tmp_img,axis=0)\n",
    "    tmp_img=preprocess_input(tmp_img)\n",
    "    batch_image.append(tmp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd03350",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f16073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76157e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation reseaux de neurones de mon modele de base\n",
    "#taille de nos images \n",
    "IMG_SHAPE = (224,224,3)\n",
    "base_model = VGG16(input_shape=IMG_SHAPE, include_top=False, pooling='avg', weights='imagenet')\n",
    "#https://keras.io/api/applications/vgg/#vgg16-function\n",
    "#input_shape=données d'entree\n",
    "#include_top est ce que je veux rajouter quelque chose a la sortie rajouter dautres \n",
    "#couches\n",
    "#pooling permet de limiter le nombre de features\n",
    "#vgg16 sortait presque 100000 et faisait bugger l'acp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1873ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature=[]\n",
    "for img in batch_image :\n",
    "    feature_np = base_model.predict(img)\n",
    "    vgg16_feature.append(feature_np.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43829eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_feature_np=np.array(vgg16_feature)\n",
    "vgg16_feature_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f508ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181baac",
   "metadata": {},
   "source": [
    "## ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui trace le graphique des eboulis des valeurs propres\n",
    "def display_scree_plot(pca):\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    #plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(), c=\"red\", marker='.')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d04bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3841aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp=0.99#nombre de composantes maximum a calculer pour l'acp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=decomposition.PCA(n_components=n_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3553098",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pca= pca.fit(vgg16_feature_np)#projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a381ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9556b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36eb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions dataset avant réduction PCA : \", vgg16_feature_np.shape)\n",
    "pca = decomposition.PCA(n_components=0.99)\n",
    "feat_pca= pca.fit_transform(vgg16_feature_np)#projection\n",
    "print(\"Dimensions dataset après réduction PCA : \", feat_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd929f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5314f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize(feat_pca).map(lambda x: [float(i) for i in x])\\\n",
    "        .toDF([str(i) for i in list(np.arange(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90390b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae128a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
