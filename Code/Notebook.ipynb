{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b26aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Set env variable\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages com.amazonaws:aws-java-sdk-pom\" \\\n",
    "                                    \":1.12.220,org.apache.hadoop:hadoop-aws:3.2.1 \" \\\n",
    "                                    \"pyspark-shell\"\n",
    "print(\"PYSPARK_SUBMIT_ARGS: \" + str(os.environ['PYSPARK_SUBMIT_ARGS']) + \"/n\")\n",
    "\n",
    "# Locate Spark\n",
    "findspark.init('/home/ubuntu/spark-3.3.0-bin-hadoop3')\n",
    "print(\"Locate Spark: \" + str(findspark.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# ML Pyspark\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "import boto3 as boto\n",
    "import io\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "# General librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librairies to deal with images\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea619ce",
   "metadata": {},
   "source": [
    "## Sommaire <a class=\"anchor\" id=\"Sommaire\"></a>\n",
    "\n",
    "* [Définition des variables globales et configurations](#Partie1)    \n",
    "* [Récupération des images depuis S3](#Partie2)\n",
    "* [Feature engineering à l'aide d'un CNN](#Partie3)\n",
    "* [Analyse en Composantes Principales (ACP)](#Partie4)\n",
    "* [Enregistrement sur le cloud](#Partie5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305718b",
   "metadata": {},
   "source": [
    "## Définition des variables globales et configurations <a class=\"anchor\" id=\"Partie1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set connection Variables\n",
    "ACCESS_KEY = \"AKIAQ6ET7AAESVDSBNPZ\"\n",
    "SECRET_KEY = \"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\"\n",
    "BUCKET_NAME = \"donnees-projet8\"\n",
    "PATH_IN_THE_BUCKET = \"Images\"\n",
    "REGION_NAME = \"eu-west-1\"\n",
    "END_POINT = \"eu-west-1.amazonaws.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3137084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Notebook P8\")\n",
    "         .config(\"spark.driver.extraJavaOptions\", \n",
    "                 \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \" \\\n",
    "                 \"--add-opens=java.base/java.lang=ALL-UNNAMED  \" \\\n",
    "                 \"--add-opens=java.base/java.util=ALL-UNNAMED\")\n",
    "         .config('spark.hadoop.fs.s3a.access.key', ACCESS_KEY)\n",
    "         .config('spark.hadoop.fs.s3a.secret.key', SECRET_KEY)\n",
    "         .config('spark.hadoop.fs.s3a.region', REGION_NAME)\n",
    "         .config('spark.hadoop.fs.s3a.endpoint', END_POINT)\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", 'org.apache.hadoop.fs.s3a.NativeS3FileSystem')\n",
    "         .config(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "         .config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "    \n",
    "         # Amount of memory to be used for the driver process\n",
    "         .config(\"spark.driver.memory\",\"16g\")\n",
    "         # Amount of memory to be used for the executor process\n",
    "         .config(\"spark.executor.memory\",\"12g\")\n",
    "         # Number of cores to be used for the executor process\n",
    "         .config(\"spark.executor.cores\",\"4\")\n",
    "         .getOrCreate())\n",
    "    \n",
    "\n",
    "# Spark context\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", END_POINT)\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ec709",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e9bbf",
   "metadata": {},
   "source": [
    "[Retour au sommaire](#Sommaire)\n",
    "### Récupération des images depuis S3 <a class=\"anchor\" id=\"Partie2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e753c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accès aux données S3\n",
    "s3 = boto.resource(\"s3\", \n",
    "                   aws_access_key_id = ACCESS_KEY, \n",
    "                   aws_secret_access_key = SECRET_KEY, \n",
    "                   region_name = REGION_NAME)\n",
    "mon_bucket = s3.Bucket(BUCKET_NAME)\n",
    "liste_images = mon_bucket.objects.filter(Prefix = PATH_IN_THE_BUCKET).all()\n",
    "\n",
    "# Lecture de toutes les images du bucket et récupération de leur catégorie\n",
    "data = []\n",
    "for obj in liste_images:\n",
    "    file_stream = obj.get()['Body']\n",
    "    img = file_stream.read()\n",
    "    file_stream.close()\n",
    "    \n",
    "    label = obj.key.split(\"/\")[-2]\n",
    "    data.append([label, img])\n",
    "\n",
    "# Définition des colonnes du dataframe\n",
    "schema = StructType([\n",
    "    StructField(\"Catégorie\", StringType(), True),\n",
    "    StructField(\"Image\", BinaryType(), True)])\n",
    "\n",
    "# Création du dataframe\n",
    "spark_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"Nombre d'images récupérées :\", str(spark_df.count()), \"\\n\")\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'une image\n",
    "img = spark_df.select('Image').collect()\n",
    "first_img = img[0][0]\n",
    "Image.open(io.BytesIO(first_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c47db",
   "metadata": {},
   "source": [
    "[Retour au sommaire](#Sommaire)\n",
    "## Feature engineering à l'aide d'un CNN <a class=\"anchor\" id=\"Partie3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False,\n",
    "                 weights=\"imagenet\",\n",
    "                 pooling=\"avg\")\n",
    "\n",
    "# verify that the top layer is removed\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1cad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=VectorUDT())\n",
    "def Vectorizer(content):\n",
    "    # Définition du CNN\n",
    "    model = ResNet50(include_top=False,\n",
    "                     weights=\"imagenet\",\n",
    "                     pooling=\"avg\")\n",
    "    \n",
    "    # Ouverture de l'image et conversion en array\n",
    "    img = Image.open(io.BytesIO(content))\n",
    "    arr_img = img_to_array(img)\n",
    "    \n",
    "    # Preprocessing de l'image\n",
    "    preprocessed_arr_img = preprocess_input(arr_img)\n",
    "\n",
    "    # Redimensionnement de l'image\n",
    "    reshaped_arr_img = preprocessed_arr_img.reshape(\n",
    "        (1,\n",
    "         preprocessed_arr_img.shape[0],\n",
    "         preprocessed_arr_img.shape[1],\n",
    "         preprocessed_arr_img.shape[2]))\n",
    "    \n",
    "    # Passage de l'image dans le CNN\n",
    "    X_features = model.predict(reshaped_arr_img)\n",
    "    \n",
    "    # Récupération des features\n",
    "    features = X_features.flatten().tolist()\n",
    "    return DenseVector(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1da320",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"CNN_Features\", Vectorizer(spark_df['Image']))\n",
    "\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_CNN = spark_df.select('CNN_Features').collect()\n",
    "len(features_CNN[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609a819",
   "metadata": {},
   "source": [
    "En sortie du CNN, chaque image est représentée par 2048 features. Pour éviter d'éventuels problèmes de mémoire lors de la classification, nous réalisons une ACP afin de réduire la dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3cfdcd",
   "metadata": {},
   "source": [
    "[Retour au sommaire](#Sommaire)\n",
    "## Analyse en Composantes Principales (ACP) <a class=\"anchor\" id=\"Partie4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495aca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the size of the Arrow record batches\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06349788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "scaler = StandardScaler(inputCol=\"CNN_Features\",\n",
    "                        outputCol=\"ScaledFeatures\",\n",
    "                        withStd=True,\n",
    "                        withMean=True)\n",
    "model_scaler = scaler.fit(spark_df)\n",
    "spark_df = model_scaler.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons 22 images donc nous allons essayer de réduire le nombres de dimensions à moins de 22\n",
    "n_comp = 15\n",
    "pca = PCA(k=n_comp, inputCol=\"ScaledFeatures\", outputCol=\"PCA_Features\")\n",
    "model_pca = pca.fit(spark_df)\n",
    "model_pca.explainedVariance.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa631a",
   "metadata": {},
   "source": [
    "En sélectionnant 15 composantes, nous expliquons 90% de la variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c38dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = model_pca.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher l'éboulis des valeurs propres\n",
    "def display_scree_plot(pca):\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"Rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"Pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n",
    "\n",
    "display_scree_plot(model_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c93efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d878c",
   "metadata": {},
   "source": [
    "[Retour au sommaire](#Sommaire)\n",
    "## Enregistrement sur le cloud <a class=\"anchor\" id=\"Partie5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pandas_df = spark_df.toPandas()\n",
    "final_pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = io.StringIO()\n",
    "final_pandas_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.Object(BUCKET_NAME, \"Sorties/Final_df.csv\").put(Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
